# -*- coding: utf-8 -*-
"""DETR_on_Kitti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u2t3ZzowYqEqPC19mVbzJb0dRSF4YzyT
"""

# Step 1: Set path.

import os
HOME = os.getcwd()
print(HOME)

# Step 2: Install necessary libraries.
# Supervision (detection), transformer (load DETR model), pytorch lightning (manage training), timm (load backbone).

!pip install -i https://test.pypi.org/simple/ supervision==0.3.0
!pip install -q transformers
!pip install -q pytorch-lightning
!pip install -q timm

# Step 3 Generate json coco format.
# Extract train & val images from train & val .txt.
# Send them to the coco function to extract image, annotation, & categories in json format.
# For annotation, parse the label to extract boxes (x, y, w, h), area, category, iscrowd to match with coco format.
# Arrange the images & labels in the folder structure acceptable by DETR, i.e., folder with subfolders train & valid with corresonding images & json.

import os
import json
from shutil import copy2
from PIL import Image

# Paths
base_path = "/content/drive/MyDrive/DETR"
image_path = os.path.join(base_path, "train", "images")
label_path = os.path.join(base_path, "train", "labels")
output_base = os.path.join(base_path, "coco_update")

train_txt_path = "/content/train.txt"
val_txt_path = "/content/val.txt"

# Output folders
output_train = os.path.join(output_base, "train")
output_valid = os.path.join(output_base, "valid")
os.makedirs(output_train, exist_ok=True)
os.makedirs(output_valid, exist_ok=True)

# COCO-style category list (class_id = 1 for COCO, but DETR internally shifts this to class_id=0)
categories = [{"id": 0, "name": "Car"}] # Ensure that this category match with id2label & label2id set in the model

def load_split(txt_path): # Train.txt, val.txt
    with open(txt_path, "r") as f:
        return [line.strip() for line in f.readlines()]

def parse_label_file(label_file):
    annotations = []
    with open(label_file, "r") as f:
        lines = f.readlines()
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 8:
                continue
            if parts[0].lower() != "car":  # Handle 'car', 'Car', etc.
                continue
            try:
                x_min = float(parts[4])
                y_min = float(parts[5])
                x_max = float(parts[6])
                y_max = float(parts[7])
                width = x_max - x_min
                height = y_max - y_min
                annotations.append({
                    "bbox": [x_min, y_min, width, height],  # COCO format
                    "area": width * height,
                    "category_id": 0,  # COCO-style ID (DETR will convert to 0 internally)
                    "iscrowd": 0,
                })
            except Exception as e:
                print(f"Skipping invalid line in {label_file}: {line}")
    return annotations

def generate_coco_json(image_ids, output_img_folder, json_output_path):
    images = []
    annotations = []
    ann_id = 1
    for i, image_file in enumerate(image_ids):
        img_name = image_file
        label_name = image_file.replace(".png", ".txt")
        img_src = os.path.join(image_path, img_name)
        lbl_src = os.path.join(label_path, label_name)

        if not os.path.exists(img_src):
            print(f"Image not found: {img_src}")
            continue

        # Copy image to output
        copy2(img_src, os.path.join(output_img_folder, img_name))

        # Open the image to get actual size
        with Image.open(img_src) as img:
            width, height = img.size

        # Image metadata
        images.append({
            "id": i,
            "file_name": img_name,
            "height": height,
            "width": width,
        })

        # Parse and add annotations
        anns = []
        if os.path.exists(lbl_src):
            anns = parse_label_file(lbl_src)

        for ann in anns:
            ann["image_id"] = i
            ann["id"] = ann_id
            ann_id += 1
            annotations.append(ann)

    # Build JSON
    coco_dict = {
        "images": images,
        "annotations": annotations,
        "categories": categories
    }

    with open(json_output_path, "w") as f:
        json.dump(coco_dict, f)

    print(f"Saved: {json_output_path}")

# Load image splits
train_ids = load_split(train_txt_path)
val_ids = load_split(val_txt_path)

# Generate COCO JSONs
generate_coco_json(train_ids, output_train, os.path.join(output_train, "_annotations.coco.json"))
generate_coco_json(val_ids, output_valid, os.path.join(output_valid, "_annotations.coco.json"))

print("COCO-style JSONs created and images copied.")

# Step 4: Load model & image processor.
# Load both image processor & object detection model (they both must have the same model).
# Image processor handles set of utilities for:
# a) Preprocessing such as image resizing, normalization, padding, conversion to tensors, and
# b) Post-processeing such as model outputs, turning raw logits into actual bounding boxes, labels, and scores.
# Set conf & iou threshold for inference/post processing

import torch
from transformers import DetrForObjectDetection, DetrImageProcessor

# Model
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
CHECKPOINT = 'facebook/detr-resnet-50'

# Processor
image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)
model = DetrForObjectDetection.from_pretrained(CHECKPOINT)
model.to(DEVICE)

# Step 5: Create train & val dataset.
# CocoDetection (parent): Loads image file and raw annotations from JSON.
# DetrImageProcessor: Preprocesses image (resizes, normalizes), converts boxes and labels into DETR format.
# Our custom class: Bridges both, uses parent to load raw data, processor to prep for model.

import os
import torchvision

# Set paths
ANNOTATION_FILE_NAME = "_annotations.coco.json"
DATASET_LOCATION = "/content/drive/MyDrive/DETR/coco_update"
TRAIN_DIRECTORY = os.path.join(DATASET_LOCATION, "train")
VAL_DIRECTORY = os.path.join(DATASET_LOCATION, "valid")

# CocoDetection (parent), apply image processor
class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, image_directory_path: str, image_processor, train: bool = True):
        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)
        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path) # Parent class' constructor
        self.image_processor = image_processor

    def __getitem__(self, idx):
        images, annotations = super(CocoDetection, self).__getitem__(idx) # Parent class' getitem
        image_id = self.ids[idx]
        annotations = {'image_id': image_id, 'annotations': annotations}
        encoding = self.image_processor(images=images, annotations=annotations, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze()
        target = encoding["labels"][0]
        return pixel_values, target

# Train & val dataset
TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY,image_processor=image_processor,train=True)
VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY,image_processor=image_processor,train=False)

print("Number of training examples:", len(TRAIN_DATASET))
print("Number of validation examples:", len(VAL_DATASET))

# Commented out IPython magic to ensure Python compatibility.
# Step 6: Example to visualize for debugging and inspection.
# Whether an image is correctly loaded.
# Whether the COCO-style annotations are properly parsed.
# Whether bounding boxes are drawn at the right locations.
# Whether class labels are being decoded and displayed correctly.

import random
import cv2
import numpy as np
import supervision as sv

# Select random image
image_ids = TRAIN_DATASET.coco.getImgIds() # Returns image metadata from the COCO dataset. It is a part of the pycocotools COCO API.
image_id = random.choice(image_ids) # Select random image_id
print('Image #{}'.format(image_id))

# Load image and annotatons
image = TRAIN_DATASET.coco.loadImgs(image_id)[0] # Get the image
annotations = TRAIN_DATASET.coco.imgToAnns[image_id] # Get the annotation
image_path = os.path.join(TRAIN_DATASET.root, image['file_name']) # Get the image path
image = cv2.imread(image_path)

# Annotate
detections = sv.Detections.from_coco_annotations(coco_annotation=annotations) # Helper class to represent a list of bounding boxes and categories.

# Use id2label function for training
categories = TRAIN_DATASET.coco.cats
d2label = {k: v['name'] for k,v in categories.items()}
id2label = {0: "car"}
label2id = {v: k for k, v in id2label.items()}
labels = [f"{id2label[class_id]}" for _, _, class_id, _ in detections]

box_annotator = sv.BoxAnnotator()
frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)

# %matplotlib inline
sv.show_frame_in_notebook(image, (16, 16))

# Step 7: Update the collate function.
# Recieve the batch, extract pixel values and labels.
# Collect images in batch, find the largest H, W in the batch, pad the images in the image to that size.
# Creates a pixel mask: 1 = valid pixel, 0 = padding (used in attention masking inside DETR).

from torch.utils.data import DataLoader

def collate_fn(batch):
    pixel_values = [item[0] for item in batch] # Each item in batch is a tuple: (pixel_values, labels)
    labels = [item[1] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")

    return {
        'pixel_values': encoding['pixel_values'],
        'pixel_mask': encoding['pixel_mask'],
        'labels': labels
    }

TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)
VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)
#TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)

# Step 5
# Plugging the HuggingFace DETR model into the Lightning pipeline.
# Example: Exposing the forward of the wrapped HuggingFace model.
# Different learning rates to improve convergence and stability.
# Resnet (ImageNet, no aggressive update) -> small learning rate to fine-tune gently.
# Transformer (need to learn actively) -> higher learning rate to adapt faster.

import pytorch_lightning as pl
from transformers import DetrForObjectDetection
import torch

id2label = {0: "car"}
label2id = {v: k for k, v in id2label.items()}

class Detr(pl.LightningModule): # DETR: subclass of pl.LightningModule to load and call the model, train, validate, and optimize it
    def __init__(self, lr, lr_backbone, weight_decay):
        super().__init__()
        self.model = DetrForObjectDetection.from_pretrained(
            pretrained_model_name_or_path=CHECKPOINT,
            num_labels=len(id2label),
            id2label=id2label,
            label2id=label2id,
            ignore_mismatched_sizes=True
        )
        self.lr = lr
        self.lr_backbone = lr_backbone
        self.weight_decay = weight_decay

    def forward(self, pixel_values, pixel_mask):
        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)

    def common_step(self, batch, batch_idx):
        pixel_values = batch["pixel_values"]
        pixel_mask = batch["pixel_mask"]
        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch["labels"]]
        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels) # Pixel value, mask, label
        loss = outputs.loss
        loss_dict = outputs.loss_dict
        return loss, loss_dict

    def training_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        self.log("training/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        for k,v in loss_dict.items():
            self.log("train_" + k, v.item())
        return loss

    def validation_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        self.log("validation/loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        for k, v in loss_dict.items():
            self.log("validation_" + k, v.item())
        return loss

    def configure_optimizers(self):
        param_dicts = [
            {
                "params": [p for n, p in self.named_parameters() if "backbone" not in n and p.requires_grad]},
            {
                "params": [p for n, p in self.named_parameters() if "backbone" in n and p.requires_grad],
                "lr": self.lr_backbone,
            },
        ]
        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)

    def train_dataloader(self):
        return TRAIN_DATALOADER

    def val_dataloader(self):
        return VAL_DATALOADER

# Commented out IPython magic to ensure Python compatibility.
# Step 8: Load tensor board for tracking the model's progress.

# %cd {HOME}
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

# Step 9: Set up TensorBoard logging for PyTorch Lightning.

from pytorch_lightning.loggers import TensorBoardLogger
logger = TensorBoardLogger("lightning_logs", name="detr")

# Step 10: Training & evaluation.

# Train the model while ensuring reproducibility, tracking loss, & setting hyperparameters.
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
import torch

# Reproducibility
def seed_everything(seed=42):
    pl.seed_everything(seed, workers=True)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
seed_everything(42)

# Define the model
model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)

# Checkpoint callback
checkpoint_callback = ModelCheckpoint(
    save_top_k=1,
    monitor="validation/loss",
    mode="min",
    dirpath="checkpoints/",
    filename="best-checkpoint",
    save_weights_only=True
)

# Trainer settings
MAX_EPOCHS = 40
trainer = Trainer(
    logger=logger,
    callbacks=[checkpoint_callback],
    devices=1,
    accelerator="gpu",
    max_epochs=MAX_EPOCHS,
    gradient_clip_val=0.1,
    accumulate_grad_batches=8,
    log_every_n_steps=5
)

# Train
trainer.fit(model)

# Step 11: Save the results.

# Save the model
MODEL_PATH_40_updated = "/content/drive/MyDrive/DETR/custom-model_40epochs_updated"
model.model.save_pretrained(MODEL_PATH_40_updated)
image_processor.save_pretrained(MODEL_PATH_40_updated)

# Optional, save the weights
torch.save(model.state_dict(), "lightning_detr_weights.pt")

# (Optional but recommended)
model = DetrForObjectDetection.from_pretrained(MODEL_PATH_40_updated).to(DEVICE)
image_processor = DetrImageProcessor.from_pretrained(MODEL_PATH_40_updated)

# Commented out IPython magic to ensure Python compatibility.
# Step 12: Inference on 1 (random) image from val dataset.

import random
import cv2
import numpy as np

# Utils
categories = VAL_DATASET.coco.cats
id2label = {k: v['name'] for k,v in categories.items()}
box_annotator = sv.BoxAnnotator()

# Select random image
image_ids = VAL_DATASET.coco.getImgIds() # get image ids
image_id = random.choice(image_ids) # choose random image id
print('Image #{}'.format(image_id))

# Load image and annotatons
image = VAL_DATASET.coco.loadImgs(image_id)[0] # extract image associated to that id
annotations = VAL_DATASET.coco.imgToAnns[image_id] # extract annoatations associated to that image
image_path = os.path.join(VAL_DATASET.root, image['file_name']) # extract image file name to get image path for that annotation
image = cv2.imread(image_path) # read the image path

# Annotate the ground truth
detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)
labels = [f"{id2label[class_id]}" for _, _, class_id, _ in detections]
frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)
print('ground truth')
# %matplotlib inline
sv.show_frame_in_notebook(frame, (16, 16))

# Inference
with torch.no_grad():
    # Load image and predict
    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE) # pre processing
    outputs = model(**inputs)

    # Post-process
    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)
    results = image_processor.post_process_object_detection( # post processing
        outputs=outputs,
        threshold=0.9,
        target_sizes=target_sizes
    )[0]

# Annotate the detection
detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=0.9)
labels = [f"{id2label[class_id]} {confidence:.2f}" for _, confidence, class_id, _ in detections]
frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)
print('detections')
# %matplotlib inline
sv.show_frame_in_notebook(frame, (16, 16))

# Step 13: Inference on all images from val dataset.

import os
import cv2
import torch
import zipfile
import numpy as np
import supervision as sv
from tqdm import tqdm

# ---- CONFIG ---- #
SAVE_DIR = '/content/DETR_inference_results_val_conf_0.9_updated'
ZIP_PATH = '/content/DETR_inference_results_val_conf_0.9_updated.zip'
THRESHOLD = 0.9

# Setup
os.makedirs(SAVE_DIR, exist_ok=True)

# Utils
categories = VAL_DATASET.coco.cats
id2label = {k: v['name'] for k, v in categories.items()}
box_annotator = sv.BoxAnnotator()

# Get all image IDs
image_ids = VAL_DATASET.coco.getImgIds()

# Inference loop
for image_id in tqdm(image_ids, desc="Processing Validation Set"):
    # Load image & annotations
    img_info = VAL_DATASET.coco.loadImgs(image_id)[0]
    annotations = VAL_DATASET.coco.imgToAnns[image_id]
    image_path = os.path.join(VAL_DATASET.root, img_info['file_name'])
    image = cv2.imread(image_path)

    if image is None:
        print(f"Warning: Could not load image {image_path}")
        continue

    # ---- GROUND TRUTH ----
    if len(annotations) > 0:
        detections_gt = sv.Detections.from_coco_annotations(coco_annotation=annotations)
        labels_gt = [f"{id2label[class_id]}" for _, _, class_id, _ in detections_gt]
        frame_gt = box_annotator.annotate(scene=image.copy(), detections=detections_gt, labels=labels_gt)
    else:
        print('No GT boxes')
        frame_gt = image.copy()  # No GT boxes

    # ---- PREDICTIONS ----
    with torch.no_grad():
        inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)
        outputs = model(**inputs)

        target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)
        results = image_processor.post_process_object_detection(
            outputs=outputs,
            threshold=THRESHOLD,
            target_sizes=target_sizes
        )[0]

    # Handle empty detection results
    if len(results["scores"]) > 0:
        detections_pred = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=0.9)
        labels_pred = [f"{id2label[class_id]} {confidence:.2f}" for _, confidence, class_id, _ in detections_pred]
        frame_pred = box_annotator.annotate(scene=image.copy(), detections=detections_pred, labels=labels_pred)
    else:
        frame_pred = image.copy()  # No predictions

    # Save both GT and predictions
    filename_base = os.path.splitext(os.path.basename(img_info['file_name']))[0]
    cv2.imwrite(os.path.join(SAVE_DIR, f"{filename_base}_gt.jpg"), frame_gt)
    cv2.imwrite(os.path.join(SAVE_DIR, f"{filename_base}_pred.jpg"), frame_pred)

# ---- ZIP the results ---- #
with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, _, files in os.walk(SAVE_DIR):
        for file in files:
            file_path = os.path.join(root, file)
            arcname = os.path.relpath(file_path, SAVE_DIR)
            zipf.write(file_path, arcname)

print(f"\n Done! Results saved to: {SAVE_DIR}")
print(f" Zipped file ready at: {ZIP_PATH}")

# Install library for evaluation

!pip install -q coco_eval

# Step 14: Evaluation via coco eval.

# Convert xywh to xyxy format.
def convert_to_xywh(boxes):
    xmin, ymin, xmax, ymax = boxes.unbind(1)
    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)

# Prepare the result for coco evaluation.
def prepare_for_coco_detection(predictions):
    coco_results = []
    for original_id, prediction in predictions.items():
        if len(prediction) == 0:
            continue

        boxes = prediction["boxes"]
        boxes = convert_to_xywh(boxes).tolist()
        scores = prediction["scores"].tolist()
        labels = prediction["labels"].tolist()

        coco_results.extend(
            [
                {
                    "image_id": original_id,
                    "category_id": labels[k],
                    "bbox": box,
                    "score": scores[k],
                }
                for k, box in enumerate(boxes)
            ]
        )
    return coco_results

# Step 15: Extract validation metrics.
# Model predictions (raw outputs -> cxcywh, normalized).
# Post-processing to get final boxes (xyxy -> absolute pixel coords), scores, and labels.
# Get results (boxes, labels, scores in real image size).
# Prepare_for_coco_detection(results), convert_to_xywh().
# Get [{"image_id", "category_id", "bbox", "score"}, ...]
# Evaluation using CocoEvaluator (evaluator.update(...), evaluator.accumulate(), summarize())

from coco_eval import CocoEvaluator
from tqdm.notebook import tqdm

import numpy as np

evaluator = CocoEvaluator(coco_gt=VAL_DATASET.coco, iou_types=["bbox"])

print("Running evaluation...")

for idx, batch in enumerate(tqdm(VAL_DATALOADER)):
    pixel_values = batch["pixel_values"].to(DEVICE)
    pixel_mask = batch["pixel_mask"].to(DEVICE)
    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch["labels"]]

    with torch.no_grad():
      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)

    orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes) # cx, cy, w, h ∈ [0, 1]  # normalized to image size to xyxy absolute pixels for drawing boxes & computing IoU

    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}
    predictions = prepare_for_coco_detection(predictions)
    evaluator.update(predictions)

evaluator.synchronize_between_processes()
evaluator.accumulate()
evaluator.summarize()